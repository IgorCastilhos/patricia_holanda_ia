# Mesa C√≥smica Arcangelina F√©

Sistema de chat com IA usando Ollama, React e Node.js.

## üöÄ Deploy na VPS Hostinger

### Pr√©-requisitos
- VPS com Docker e Docker Compose instalados
- Portas 80, 3001 e 11434 liberadas no firewall
- Git instalado

### Passo a Passo

1. **Clone o reposit√≥rio na VPS:**
```bash
git clone <seu-repositorio>
cd teste_ia
```

2. **Inicie os containers:**
```bash
docker compose up -d --build
```

3. **Aguarde os containers iniciarem (cerca de 1-2 minutos) e inicialize o modelo Ollama:**
```bash
chmod +x init-ollama.sh
./init-ollama.sh
```

4. **Verifique o status:**
```bash
docker compose ps
```

### üîç Comandos √öteis

**Ver logs:**
```bash
# Todos os servi√ßos
docker compose logs -f

# Apenas um servi√ßo
docker compose logs -f backend
docker compose logs -f frontend
docker compose logs -f ollama
```

**Reiniciar servi√ßos:**
```bash
docker compose restart
```

**Parar todos os servi√ßos:**
```bash
docker compose down
```

**Parar e remover volumes:**
```bash
docker compose down -v
```

**Acessar o shell de um container:**
```bash
docker exec -it <container-name> sh
```

### üåê Acessando a Aplica√ß√£o

Ap√≥s o deploy, acesse:
- **Frontend:** http://seu-ip-ou-dominio
- **Backend API:** http://seu-ip-ou-dominio/api/chat
- **Ollama:** http://seu-ip-ou-dominio:11434

### üîß Troubleshooting

**Se o modelo n√£o funcionar:**
```bash
# Entre no container do Ollama
docker exec -it $(docker ps -qf "name=ollama") sh

# Liste os modelos
ollama list

# Recrie o modelo
ollama create arcangelina -f /tmp/Modelfile
```

**Se o backend n√£o conectar ao Ollama:**
```bash
# Verifique se o container do Ollama est√° rodando
docker ps | grep ollama

# Verifique os logs
docker compose logs ollama
```

**Problemas de build:**
```bash
# Limpe tudo e reconstrua
docker compose down -v
docker system prune -a
docker compose up -d --build
```

### üì¶ Estrutura dos Servi√ßos

- **Frontend (porta 80):** Interface React com Nginx
- **Backend (porta 3001):** API Node.js/Express com TypeScript
- **Ollama (porta 11434):** Motor de IA com modelo customizado

### üîê Seguran√ßa (Recomenda√ß√µes)

Para produ√ß√£o, considere:
1. Usar HTTPS com certificado SSL (Let's Encrypt)
2. Configurar firewall (ufw/iptables)
3. Usar vari√°veis de ambiente para secrets
4. Implementar rate limiting
5. Configurar backup dos volumes Docker

### üìù Vari√°veis de Ambiente

O backend usa as seguintes vari√°veis (definidas no docker-compose.yml):
- `OLLAMA_HOST`: URL do servi√ßo Ollama
- `OLLAMA_MODEL`: Nome do modelo a ser usado

### üîÑ Atualiza√ß√µes

Para atualizar a aplica√ß√£o:
```bash
git pull
docker compose up -d --build
```

## üíª Desenvolvimento Local

```bash
# Backend
cd backend
npm install
npm start

# Frontend
cd frontend
npm install
npm run dev

# Ollama (localmente)
ollama pull llama3
ollama create arcangelina -f ../Modelfile
```

