# Mesa CÃ³smica Arcangelina FÃ©

Sistema de chat com IA usando Ollama, React e Node.js.

## ğŸš€ Deploy na Hostinger (ou qualquer VPS)

### âœ¨ Processo AutomÃ¡tico

Este projeto estÃ¡ configurado para **inicializaÃ§Ã£o 100% automÃ¡tica**! 

Quando vocÃª enviar o repositÃ³rio para a Hostinger e ela executar o `docker-compose up`, o sistema vai:

1. âœ… Subir o container do Ollama
2. âœ… **Criar automaticamente o modelo "arcangelina"** usando o Modelfile
3. âœ… Subir o backend conectado ao Ollama
4. âœ… Subir o frontend

**NÃ£o Ã© necessÃ¡rio executar nenhum comando shell manual!** ğŸ‰

### PrÃ©-requisitos na VPS
- Docker e Docker Compose instalados
- Portas 80, 3001 e 11434 liberadas no firewall

### Como Funciona

O `docker-compose.yml` foi configurado para:
- Montar o `Modelfile` dentro do container do Ollama
- Executar automaticamente um script de inicializaÃ§Ã£o que:
  - Verifica se o modelo "arcangelina" existe
  - Se nÃ£o existir, cria o modelo usando `ollama create arcangelina -f /tmp/Modelfile`
  - MantÃ©m o serviÃ§o rodando

### ğŸ” Comandos Ãšteis (Opcional)

**Ver logs para acompanhar a criaÃ§Ã£o do modelo:**
```bash
docker compose logs -f ollama
```

**Ver status de todos os serviÃ§os:**
```bash
docker compose ps
```

**Reiniciar todos os serviÃ§os:**
```bash
docker compose restart
```

**Parar todos os serviÃ§os:**
```bash
docker compose down
```

### ğŸŒ Acessando a AplicaÃ§Ã£o

ApÃ³s o deploy, acesse:
- **Frontend:** http://seu-ip-ou-dominio
- **Backend API:** http://seu-ip-ou-dominio:3001/api/chat
- **Ollama:** http://seu-ip-ou-dominio:11434

### ğŸ”§ Troubleshooting

**Se o chat nÃ£o responder imediatamente:**
- O modelo pode estar sendo criado (leva 1-3 minutos na primeira vez)
- Verifique os logs: `docker compose logs -f ollama`
- Aguarde a mensagem "âœ¨ Modelo 'arcangelina' criado com sucesso!"

**Para verificar se o modelo foi criado:**
```bash
docker exec -it $(docker ps -qf "name=ollama") ollama list
```

VocÃª deve ver "arcangelina" na lista.

**Para recriar o modelo (se necessÃ¡rio):**
```bash
docker compose down
docker volume rm teste_ia_ollama_data
docker compose up -d --build
```
ollama create arcangelina -f /tmp/Modelfile
```

**Se o backend nÃ£o conectar ao Ollama:**
```bash
# Verifique se o container do Ollama estÃ¡ rodando
docker ps | grep ollama

# Verifique os logs
docker compose logs ollama
```

**Problemas de build:**
```bash
# Limpe tudo e reconstrua
docker compose down -v
docker system prune -a
docker compose up -d --build
```

### ğŸ“¦ Estrutura dos ServiÃ§os

- **Frontend (porta 80):** Interface React com Nginx
- **Backend (porta 3001):** API Node.js/Express com TypeScript
- **Ollama (porta 11434):** Motor de IA com modelo customizado

### ğŸ” SeguranÃ§a (RecomendaÃ§Ãµes)

Para produÃ§Ã£o, considere:
1. Usar HTTPS com certificado SSL (Let's Encrypt)
2. Configurar firewall (ufw/iptables)
3. Usar variÃ¡veis de ambiente para secrets
4. Implementar rate limiting
5. Configurar backup dos volumes Docker

### ğŸ“ VariÃ¡veis de Ambiente

O backend usa as seguintes variÃ¡veis (definidas no docker-compose.yml):
- `OLLAMA_HOST`: URL do serviÃ§o Ollama
- `OLLAMA_MODEL`: Nome do modelo a ser usado

### ğŸ”„ AtualizaÃ§Ãµes

Para atualizar a aplicaÃ§Ã£o:
```bash
git pull
docker compose up -d --build
```

## ğŸ’» Desenvolvimento Local

```bash
# Backend
cd backend
npm install
npm start

# Frontend
cd frontend
npm install
npm run dev

# Ollama (localmente)
ollama pull llama3
ollama create arcangelina -f ../Modelfile
```

