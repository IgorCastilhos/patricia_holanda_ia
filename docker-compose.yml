version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    networks:
      - app-network
    restart: unless-stopped

  backend:
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=arcangelina
    depends_on:
      - ollama
    networks:
      - app-network
    restart: unless-stopped
    # Health check (opcional, descomente se desejar)
    # healthcheck:
    #   test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3001/api/health"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 40s

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./Modelfile:/tmp/arcangelina.Modelfile:ro
      - ./init-ollama.sh:/tmp/init-ollama.sh:ro
    networks:
      - app-network
    restart: unless-stopped
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        # Inicia o Ollama em background
        /bin/ollama serve &
        
        # Aguarda o Ollama ficar pronto
        echo "ðŸŒŒ Aguardando Ollama iniciar..."
        sleep 15
        
        # Verifica e cria o modelo se necessÃ¡rio
        echo "âœ¨ Verificando se o modelo 'arcangelina' jÃ¡ existe..."
        if ! ollama list | grep -q "arcangelina"; then
          echo "ðŸŒŸ Criando modelo 'arcangelina' a partir do Modelfile..."
          ollama create arcangelina -f /tmp/arcangelina.Modelfile
          echo "âœ¨ Modelo 'arcangelina' criado com sucesso!"
        else
          echo "ðŸ”® Modelo 'arcangelina' jÃ¡ existe!"
        fi
        
        echo "ðŸ”® Modelos disponÃ­veis:"
        ollama list
        
        # MantÃ©m o container rodando
        wait

volumes:
  ollama_data:

networks:
  app-network:
    driver: bridge
