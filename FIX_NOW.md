# üÜò Corre√ß√£o dos Erros Atuais

## Problemas Identificados

### 1. ‚ùå Modelo 'arcangelina' n√£o encontrado
```
ResponseError: model 'arcangelina' not found
```

### 2. ‚ùå Conflito de headers HTTP
```
upstream sent "Content-Length" and "Transfer-Encoding" headers at the same time
```

---

## ‚úÖ Solu√ß√µes Aplicadas

### 1. **Corre√ß√£o do Backend** (server.ts)
- Removido header `Transfer-Encoding` manual
- Adicionados headers corretos para streaming
- Melhorado tratamento de erro quando modelo n√£o existe

### 2. **Corre√ß√£o do Nginx** (nginx.conf)
- Configura√ß√£o otimizada para streaming
- Adicionado `chunked_transfer_encoding on`
- Removido conflito de headers

### 3. **Scripts de Configura√ß√£o**
- `setup-model.sh` - Configura o modelo Ollama
- `fix-complete.sh` - Aplica todas as corre√ß√µes

---

## üöÄ Como Aplicar na VPS (AGORA)

### Op√ß√£o 1: Autom√°tica (Recomendado)

```bash
cd ~/teste_ia  # ou o diret√≥rio do seu projeto
git pull       # atualizar c√≥digo
chmod +x fix-complete.sh
./fix-complete.sh
```

### Op√ß√£o 2: Manual

```bash
# 1. Parar e rebuild
docker compose down
docker compose up -d --build

# 2. Aguardar containers iniciarem
sleep 20

# 3. Configurar modelo
chmod +x setup-model.sh
./setup-model.sh
```

### Op√ß√£o 3: Apenas criar o modelo (se j√° fez rebuild)

```bash
chmod +x setup-model.sh
./setup-model.sh
```

---

## üîç Verifica√ß√£o

### 1. Verificar se modelo foi criado

```bash
docker exec $(docker compose ps -q ollama) ollama list
```

Deve aparecer:
```
NAME            ID              SIZE    MODIFIED
arcangelina:... ...             ...     ... seconds ago
llama3:...      ...             ...     ... ago
```

### 2. Testar modelo

```bash
docker exec -it $(docker compose ps -q ollama) ollama run arcangelina "Ol√°"
```

### 3. Testar API

```bash
curl -X POST http://localhost/api/health
```

### 4. Ver logs

```bash
docker compose logs -f backend
```

---

## üìã O que foi alterado

### backend/src/server.ts
```typescript
// ANTES
res.setHeader('Transfer-Encoding', 'chunked');

// DEPOIS
res.setHeader('Cache-Control', 'no-cache');
res.setHeader('Connection', 'keep-alive');
res.flushHeaders();
```

### frontend/nginx.conf
```nginx
# ADICIONADO
chunked_transfer_encoding on;
proxy_cache off;
proxy_set_header Transfer-Encoding $http_transfer_encoding;
```

---

## ‚è±Ô∏è Tempo de Aplica√ß√£o

- Op√ß√£o 1 (Autom√°tica): ~3-5 minutos
- Op√ß√£o 2 (Manual): ~2-3 minutos  
- Op√ß√£o 3 (Apenas modelo): ~1-2 minutos

---

## üÜò Se ainda der erro

### Erro: Container ollama n√£o est√° rodando
```bash
docker compose up -d ollama
sleep 10
./setup-model.sh
```

### Erro: Modelfile n√£o encontrado
```bash
# Verificar se existe
ls -la Modelfile

# Se n√£o existir, est√° na raiz do projeto
```

### Erro: Ollama n√£o responde
```bash
# Reiniciar Ollama
docker compose restart ollama
sleep 15
./setup-model.sh
```

### Erro 502 ainda aparece
```bash
# Rebuild completo
docker compose down -v
docker compose up -d --build
sleep 30
./setup-model.sh
```

---

## üìä Comandos de Debug

```bash
# Ver todos os logs
docker compose logs

# Logs apenas do backend
docker compose logs backend | tail -50

# Logs apenas do ollama
docker compose logs ollama | tail -50

# Status dos containers
docker compose ps

# Entrar no container do Ollama
docker exec -it $(docker compose ps -q ollama) sh
```

---

## ‚úÖ Checklist P√≥s-Corre√ß√£o

- [ ] Containers rodando: `docker compose ps`
- [ ] Modelo criado: `docker exec $(docker compose ps -q ollama) ollama list`
- [ ] Backend respondendo: `curl http://localhost/api/health`
- [ ] Sem erro 502 nos logs
- [ ] Chat funcionando no navegador

---

**Aplique as corre√ß√µes agora e teste o chat!** üåü

